# -*- coding: utf-8 -*-
"""PYTN_KampusMerdeka_fp4_<nama>

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pE218nD36SOFt3PY7-wwtDh8lC0pZat_

# Final Project 4 : CLUSTERING
Nama Anggota Kelompok :
- MUHAMAD ADITYA DARMAWAN (PYTN-KS18-01)
- AUDITA BELLA INTAN PUSPITA (PYTN-KS18-05)
- IVANKA STELLA AUDRIA (PYTN-KS18-08)

# PENDAHULUAN

## a. Latar Belakang
Bank adalah lembaga keuangan yang bertugas menghimpun dana dari masyarakat dan menyalurkan kembali dana tersebut ke masyarakat serta memberikan jasanya dalam alur pembayaran dan peredaran uang. Pada umumnya Bank dikenal sebagai lembaga keuangan yang kegiatan utamanya menerima simpanan, giro, tabungan dan deposito. Bank juga dikenal sebagai tempat untuk meminjam uang (kredit) bagi masyarakat yang membutuhkannya. Dengan adanya lembaga keuangan maupun non keuangan yang menjadi pilar perekonomian di Indonesia menjadikan banyak lembaga keuangan yang tumbuh didaerah daerah. Bank Perkreditan Rakyat merupakan salah satu lembaga keuangan yang ada di Indonesia.

Persaingan antar-bank saat ini menjadi sangat tinggi membuat setiap perusahaan harus memiliki strategi pemasaran produk perbankannya. Segmentasi pasar adalah salah satu strategi dalam dunia bisnis dengan mengelompokkan produk yang dimiliki sesuai dengan kesamaan, kemiripan, minat serta kebutuhan pelanggan. Alternatif strategi terhadap kegiatan pemasaran yang dilakukan sangat diperlukan agar perusahaan semakin tumbuh dan berkembang. Strategi pemasaran yang tepat dengan menetapkan segementasi pasar yang sesuai sasaran akan mempengaruhi pertumbuhan kreditnya.

## b. Dataset
Dataset yang digunakan pada analisis ini yaitu credit card dataset yang diunduh dari kaggle melalui [link berikut](https://www.kaggle.com/datasets/arjunbhasin2013/ccdata)

Data ini berisi transaksi pengguna kartu kredit sebanyak 9000 orang selama 6 bulan. Data ini memiliki 18 atribut yaitu

1. CUSTID - Identifikasi pemegang kartu kredit (Categorical)
2. BALANCE - Jumlah saldo yang tersisa di akun mereka untuk melakukan pembelian
3. BALANCE_FREQUENCY - Seberapa sering Saldo diperbarui, skor antara 0 dan 1 (1 = sering diperbarui, 0 = tidak sering diperbarui)
4. CHASES : Jumlah pembelian yang dilakukan dari akun
5. ONEOFF_PURCHASES : Jumlah pembelian maksimum yang dilakukan dalam sekali jalan
6. INSTALLMENTS_PURCHASES : Jumlah pembelian yang dilakukan dengan cicilan
7. CASH_ADVANCE : Uang muka yang diberikan oleh pengguna
8. PURCHASES_FREQUENCY : Seberapa sering Pembelian dilakukan, skor antara 0 dan 1 (1 = sering dibeli, 0 = tidak sering dibeli)
9. ONEOFFPURCHASESFREQUENCY : Seberapa sering Pembelian dilakukan sekaligus (1 = sering dibeli, 0 = tidak sering dibeli)
10. PURCHASESINSTALLMENTSFREQUENCY : Seberapa sering pembelian dengan cicilan dilakukan (1 = sering dilakukan, 0 = tidak sering dilakukan)
11. CASHADVANCEFREQUENCY : Seberapa sering cash in advance dibayar
12. CASHADVANCETRX : Jumlah Transaksi yang dilakukan dengan "Cash in Advanced"
13. PURCHASES_TRX : Jumlah transaksi pembelian yang dilakukan
14. CREDIT_LIMIT : Batas kartu kredit untuk pengguna
15. PAYMENTS : Jumlah Pembayaran yang dilakukan oleh pengguna

## c. Objective yang Ingin Dicapai

Objective yang ingin dicapai dalam analisis ini yaitu:

- Mampu memahami konsep Clustering dengan menggunakan Scikit-Learn
- Mampu mempersiapkan data untuk digunakan dalam Clustering
- Mampu mengimplementasikan Clustering pada data yang diberi

# IMPORT LIBRARY
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
import warnings

from sklearn.preprocessing import StandardScaler, normalize
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import DBSCAN

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from umap import UMAP
import umap.plot

import pickle

"""# DATA LOADING
Bagian ini berisi proses data loading yang kemudian dilanjutkan dengan explorasi data secara sederhana.
"""

df = pd.read_csv("CC GENERAL.csv")

#Melakukan setting untuk menampilkan semua kolom pada dataframe
pd.set_option("display.max_columns", None)

df

#melihat dimesi data
df.shape

#Melihat informasi dataset
df.info()

#melihat rincian kolom
df.columns

#mengecek missing value
df.isnull().values.any()

"""output menunjukkan 'True' yang berarti pada dataset masih terdapat missing value"""

#melihat statistik deskriptif data tipe numerik
df.describe()

#melihat statistik deskriptif data tipe objek
df.describe(include='O').T

#Melihat banyaknya unique values pada masing-masing atribut
for col in df.columns:
    print(col,':', df[col].nunique())

df['TENURE'].unique()

"""# DATA CLEANING

Bagian ini mencakup langkah-langkah persiapan data, yang melibatkan pembersihan data sebelum melanjutkan ke tahap eksplorasi lebih lanjut. Proses pembersihan dapat mencakup tindakan seperti memberikan nama baru pada setiap kolom, mengisi nilai yang hilang, menghapus kolom yang tidak diperlukan, dan tindakan lainnya.
"""

#Melihat apakah terdapat missing values / nilai yang hilang pada tiap kolom
df.isnull().sum()

"""Terlihat bahwa terdapat missing value pada kolom CREDIT_LIMIT dan MINIMUM_PAYMENTS. Karena pada kolom CREDIT_LIMIT hanya terdapat satu missing value, sehingga dapat dihapus saja. Sedangkan untuk missing value pada kolom MINIMUM_PAYMENTS akan diisi dengan nilai mediannya."""

#Melihat berapa persen missing value pada setiap kolom
missing_value = df.apply(lambda x: f'{((x.isnull().sum()/df.shape[0])*100).round(2)} %')
missing_value

#membuat visualisasi banyaknya missing value
df.isnull().sum().plot.bar()
plt.show()

"""dapat dilihat kolom `CREDIT_LIMIT` dan `MINIMUM_PAYMENTS` memiliki missing value"""

#Melakukan penghapusan kolom `CUST_ID` karena tidak diperlukan
df = df.drop('CUST_ID', axis=1)
df.head()

#Mengisi missing values pada kolom dengan median
df['MINIMUM_PAYMENTS'].fillna(df['MINIMUM_PAYMENTS'].median(),inplace=True)
df['CREDIT_LIMIT'].fillna(df['CREDIT_LIMIT'].median(),inplace=True)
df.head()

df.isnull().values.any()

"""setelah dilakukan penanganan missing value didapakan sudah tidak ada missing value lagi pada dataset"""

#Memberikan nama baru pada untuk setiap kolom atau atribut
df = df.rename(columns={'BALANCE':'Saldo', 'BALANCE_FREQUENCY':'FrekuensiSaldo', 'PURCHASES':'Pembelian',
                        'ONEOFF_PURCHASES':'PembelianOneoff', 'INSTALLMENTS_PURCHASES':'PembelianAngsuran',
                        'CASH_ADVANCE':'PenarikanTunai', 'PURCHASES_FREQUENCY':'FrekuensiPembelian',
                        'ONEOFF_PURCHASES_FREQUENCY':'FrekuensiPembelianOneoff',
                        'PURCHASES_INSTALLMENTS_FREQUENCY':'FrekuensiPembelianAngsuran',
                        'CASH_ADVANCE_FREQUENCY':'FrekuensiPenarikanTunai', 'CASH_ADVANCE_TRX':'PenarikanTunaiTRX',
                        'PURCHASES_TRX':'PembelianTRX', 'CREDIT_LIMIT':'BatasKredit', 'PAYMENTS':'Pembayaran',
                        'MINIMUM_PAYMENTS':'MinimalPembayaran', 'PRC_FULL_PAYMENT':'PembayaranFullPRC',
                        'TENURE':'JangkaWaktu'})

df.head()

df.columns

#Melihat banyaknya nilai dari masing-masing atribut
for col in df.columns:
    print(col, ': ')
    print(df[col].value_counts())
    print('\n','*'*60,'\n')

"""# EKSPLORASI DATA
Bagian ini berisi explorasi data pada dataset diatas dengan menggunakan query, grouping, visualisasi sederhana, dan lain sebagainya.
"""

#Melihat statistik deskriptif pada dataset
df.describe()

"""Terlihat bahwa dalam dataset:

- Saldo/Balance rata-rata yaitu 1564 dan saldo maksimal yaitu 19043.
- Frekuensi Saldo memiliki skor rata-rata 0,87
- Pembelian/Purchases dengan maksimal 49039
- Batas kredit dengan minimal 50 dan maksimal 30000
- Pembayaran rata-rata yairu 1733.
"""

#Mencari Pembayaran minimal dan maksimal untuk Jangka waktu layanan kartu kredit untuk pengguna 12 bulan
df.loc[df['JangkaWaktu'] == 12, 'Pembayaran'].agg(('min', 'max'))

"""Dari hasil diketahui bahwa dengan  untuk Jangka waktu layanan kartu kredit untuk pengguna 12 bulan pembayaran minimal yaitu 0 dan pembayaran maksimal yaitu 50721.48336 (50721)"""

#Mencari Penarikan Tunai terbesar dan terkecil
df.max(axis=0)['PenarikanTunai']

df.min(axis=0)['PenarikanTunai']

"""didapatkan penarikan tunai terbesar yaitu 47137.21176 dan penarikan tunai terkecil yaitu 0"""

#Melihat proporsi data pada kolom Jangka Waktu dan Frekuensi Saldo dengan menggunakan crosstab
pd.crosstab(df.JangkaWaktu, df.FrekuensiSaldo)

#Mengidentifikasi rata-rata (mean) Pembelian dengan melakukan grouping berdasarkan Jangka Waktu.
df_mean = df[['JangkaWaktu','Pembelian']].groupby('JangkaWaktu').mean().sort_values(by='Pembelian', ascending=False)
df_mean.T

"""Dapat dilihat Jangka waktu layanan kartu kredit untuk pengguna 12 bulan memiliki rata-rata pembelian terbesar yaitu 1088.192402 dan Jangka waktu layanan kartu kredit untuk pengguna 7 bulan memiliki rata-rata pembelian terkecil yaitu 424.559421"""

#Melihat data histori top 5 record dengan melakukan query, yang di mana: Jangka Waktu yaitu 6, Banyak saldo 0, Pembelian sebesar 0
df[
    (df['JangkaWaktu'] == 12) &
    (df['Saldo'] == 0.0) &
    (df['Pembelian'] == 0.0)
].head()

#Visualisasi line plot korelasi antara Saldo dan Pembelian
sns.lineplot(x='MinimalPembayaran',y='Pembayaran', data=df)
plt.title('Korelasi antara Saldo dan Pembelian')
plt.show()

"""Dapat disimpulkan bahwa Pembayaran selaju dengan MinimalPembayaran. Adapun anomali pada MinimalPembayaran ada pada 40000 - 80000"""

#Melihat persebaran data kolom Jangka waktu layanan kartu kredit untuk pengguna dengan bar plot
df['JangkaWaktu'].value_counts().plot.bar(color='grey')
plt.title('Persebaran Jumlah Pengguna Berdasarkan Jangka waktu layanan kartu kredit untuk pengguna')
plt.show()

"""Dapat dilihat persebaran data jumlah pengguna dengan Jangka waktu layanan kartu kredit untuk pengguna 12 bulan sangat banyak dengan jumlah diatas 7000, sedangakan jumlah pengguna dengan Jangka waktu layanan kartu kredit untuk pengguna 9 - 11 bulan rata-rata ada diangka kurang dari 1000"""

#Visualisasi top 5 jumlah pembelian tertinggi pengguna
df_v = pd.DataFrame(df['Pembelian'].value_counts())
plot = df_v.head().plot.pie(y='Pembelian', figsize=(8, 8), autopct='%1.0f%%');

plt.title("Pie Chart Top 5 jumlah pembelian tertinggi pengguna")
plt.show()

"""dapat dilihat bahwa 5 jumlah pembelian tertinggi pengguna diantaranya 0.0, 45.65, 150.0, 60.0, dan 200.0"""

#Visualisasi boxplot perbandingan Jangka waktu layanan kartu kredit untuk pengguna dan Batas Kredit
fig, ax = plt.subplots(figsize=(9, 5))

sns.boxplot(x='JangkaWaktu', y='BatasKredit', data=df, palette="spring")

plt.xlabel("JangkaWaktu", fontsize= 12)
plt.ylabel("Batas Kredit", fontsize= 12)
plt.title("JangkaWaktu vs Batas Kredit", fontsize= 15)

"""Dapat dilihat semua Jangka waktu layanan kartu kredit untuk pengguna memiliki outliers. Untuk Jangka waktu layanan kartu kredit untuk pengguna selama 12 bulan memiliki outliers yang sangat banyak sedangkan Jangka waktu layanan kartu kredit untuk pengguna selama 9 bulan memilikioutlierspaling sedikit."""

#Visualisasi boxplot perbandingan Jangka waktu layanan kartu kredit untuk pengguna dan Frekuensi saldo
fig, ax = plt.subplots(figsize=(9, 5))

sns.boxplot(x='JangkaWaktu', y='FrekuensiSaldo', data=df, palette="summer")

plt.xlabel("JangkaWaktu", fontsize= 12)
plt.ylabel("Frekuensi saldo", fontsize= 12)
plt.title("JangkaWaktu vs Frekuensi saldo", fontsize= 15)

"""Dapat dilihat Jangka waktu 6, 8 dan 9 memiliki outliers sediki, sedangkan jangka waktu 11 dan 12 bulan memiliki outliers yang cukup banyak"""

#Visualisasi scatterplot antara saldo dan pembelian
sns.scatterplot(x='Saldo', y='Pembelian', data=df, color='brown')
plt.title("Saldo vs Pembelian", fontsize= 15)

"""Dari plot dapat dilihat bahwa rata-rata pengguna kartu kredit memiliki saldo diantara 0 - 12500 dengan banyak pembelian rata-rata antara 0 - 10000. terlihat ada satu pengguna dengan pembelian yang tinggi hapir 50000 namun dengan saldo sekitar 12000."""

#Visualisasi violinplot antara kolom JangkaWaktu dan kolom BatasKredit
plt.figure(figsize=(9,7))
ax = plt.axes()
ax.set_facecolor('darkgrey')
sns.violinplot(x='JangkaWaktu', y='BatasKredit', data=df, inner='quartile')
plt.xlabel('JangkaWaktu')
plt.ylabel('BatasKredit')
plt.title('Batas Kredit selama Jangka Waktu Pengguna Credit')
plt.show()

#Visualisasi histogram masing-masing kolom data
num_col = df.columns
df[num_col].hist(bins=15, figsize=(20, 15), layout=(5, 4))

#Melakukan visualisasi korelasi antar variabel untuk melihat hubungan
df.corr()

plt.figure(figsize=(16, 9))
sns.heatmap(df.corr(), annot=True, cmap='cubehelix')
plt.show()

"""Dalam heatmap tersebut, terlihat bahwa terdapat kolerasi yang cukup tinggi antar kolom diantaranya :
- Kolom Pembelian dengan kolom PembelianOneoff dengan korelasi 0.92
- Kolom FrekuensiPembelian dengan kolom FrekuensiPembelianAngsuran dengan korelasi 0.86
- Kolom FrekuensiPenarikanTunai dengan kolom PenarikanTunaiTRX dengan korelasi 0.8

Dalam heatmap tersebut, juga terlihat bahwa terdapat kolerasi yang cukup rendah antar kolom diantaranya :
- Kolom Saldo dengan kolom PembayaranFullPRC dengan korelasi -0.32
- Kolom FrekuensiPembelian dengan kolom FrekuensiPenarikanTunai dengan korelasi -0.31
- Kolom FrekuensiPembelianAngsuran dengan kolom FrekuensiPenarikanTunsi dengan korelasi -0.26

# DATA PREPROCESSING

Proses untuk menyiapkan data untuk proses pelatihan model dibahas dalam bagian ini. Ini termasuk proses seperti membagi data menjadi tes pelatihan desainer, melakukan transformasi data (seperti normalisasi, encoding, dll.).

## a. Standarisasi pada Kata

standarisasi merupakan sebuah teknik lain dalam melakukan perubahan skala, dimana data yang dimiliki akan diubah sehingga memiliki rata rata = 0 (terpusat) dan standar deviasi = 1
"""

# Standardize data
scaler = StandardScaler()
scaled = scaler.fit_transform(df)

"""## b. Melihat statistik dari data yang sudah di standarisasi"""

# Statistics of scaled data
scaled_df = pd.DataFrame(scaled, columns=df.columns)
scaled_df.head()

"""## c. Hierarchical Clustering Dendrogram

Hierarchical Clustering adalah salah satu algoritma clustering yang dapat digunakan untuk meng-cluster dokumen (document clustering). Hasil keseluruhan dari algoritma hierarchical clustering secara grafik dapat digambarkan sebagai tree, yang disebut dengan dendogram.
"""

hier_cluster = linkage(scaled_df, method='ward')

plt.figure(figsize=(10,9))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Observations')
plt.ylabel('Distance')
dendrogram(hier_cluster, truncate_mode='level', p = 5, show_leaf_counts=False, no_labels=True)
plt.show()

"""Hasil Dendrogram Hierarchical Clustering menunjukkan bahwa ada sekitar empat clade yang berada di atas ketinggian 125. Jika Anda menggambar garis horizontal, Anda akan menemukan empat kelompok umum (cluster) pada data.

# PENDEFINISIAN MODEL

Untuk mendefinisikan model sampai kompilasi, bagian ini berisi cell untuk memberikan penjelasan, seperti mengapa memilih arsitektur atau jenis model tertentu, mengapa memilih nilai hyperparameter, dan topik lainnya.

Dalam hal ini, kami menggunakan model clustering K-Means dan PCA.

## a. K-Means Clustering

Alasan penggunaan model ini yaitu karena K-means adalah centroid-based algorithm, atau distance-based algorithm, dimana kita menghitung jarak untuk menetapkan titik ke sebuah cluster. Di K-Means, setiap cluster dikaitkan dengan centroid.
"""

Kmean = KMeans(n_clusters=2)
Kmean

"""jumlah komponen sebanyak 2 (n_comonents=2) akan dikelompokkan dengan beberapa komponen mewakili dimensi yang lebih rendah di mana hal tersebut akan memproyeksikan data dimensi yang lebih tinggi. Hal ini adalah hasil dari metode K-Means Clustering.

## b. PCA (Principal Component Analysis)

PCA dapat digunakan untuk memproyeksikan informasi dari ruang dimensi besar ke subruang yang lebih kecil dan mempertahankan bagian penting dengan lebih banyak variasi data daripada menghapus bagian yang tidak penting dengan variasi yang lebih sedikit. Ini adalah alasan mengapa model ini digunakan. untuk menjadi lebih mudah untuk melihat dan menafsirkan data yang terbagi menjadi beberapa cluster.
"""

# PCA untuk dua Komponen Utama
pca = PCA()
pca

"""Pada metode PCA, komponen dikirim sesuai dengan default dan mengikuti nilai variance. Beberapa komponen menunjukkan dimensi yang lebih rendah, dari mana kami memproyeksikan data dimensi yang lebih tinggi.

# PELATIHAN MODEL
Cell pada bagian ini hanya berisi code untuk melatih model dan output yang dihasilkan.

## a. K-Means Clustering
"""

kmeansCluster = Kmean.fit(scaled_df)

kmeansCluster

"""## b. PCA (Principal Component Analysis)

"""

principalComponents = pca.fit_transform(scaled_df)

principalComponents

"""# EVALUASI MODEL
Pada bagian ini, dilakukan evaluasi model yang harus menunjukkan bagaimana performa model berdasarkan metrics yang dipilih. Hal ini harus dibuktikan dengan visualisasi tren performa dan/atau tingkat kesalahan model.

## a. K-Means Clustering

- Melihat jumlah clusters yang opttimal degan menggunakan elbow method
"""

wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(scaled_df)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') #within cluster sum of squares
plt.show()

"""- Melatih model KMeans dengan n_clusters sebanyak 4"""

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=3)
kmeans.fit(scaled_df)

"""- Menambahkan KMeans Segment pada dataframe"""

data_kmeans = df.copy()
data_kmeans['KMeans Segment'] = kmeans.labels_

"""- Melihat transformasi data KMeans dengan melakukan groupby berdasarkan rata-rata KMeans segment"""

data_transf_kmeans = data_kmeans.groupby('KMeans Segment').mean()
data_transf_kmeans

"""Kami kemudian mencoba membuat kesimpulan tentang segmen dan memberi namanya berdasarkan hasil grouping segment KMeans.  

Di sini, Penarikan Tunai berarti menggunakan kartu kredit untuk melakukan penarikan tunai, yang biasanya memerlukan biaya dan tingkat bunga.

- Memberi nama segment
"""

data_transf_kmeans.rename({0:'Pengguna Level Tengah (Middle Ground)',
                           1:'Penggua dengan Pembelian Kredit Tinggi (High Credit Frequent Purchasers)',
                           2:'Pengguna dengan Penarikan Tunai Tinggi (High Cash Advance Users)',
                           3:'Pengguna Kredit Hemat (Frugal Credit Users)'})

"""- Memasukkan Label KMeans Segment ke dalam dataframe dengan membuat kolom baru"""

data_kmeans['Labels'] = data_kmeans['KMeans Segment'].map({0:'Pengguna Level Tengah (Middle Ground)',
                                                           1:'Pengguna dengan Kredit Pembelian Tinggi (High Credit Frequent Purchasers)',
                                                           2:'Pengguna dengan Penarikan Tunai Tinggi (High Cash Advance Users)',
                                                           3:'Pengguna Kredit Hemat (Frugal Credit Users)'})

data_kmeans.head()

"""**Analisis Segment K-Means**

Pada tahap ini, plotting atribut pada segment KMeans dilakukan untuk membagi data dan mengidentifikasi perbedaan antara segmen pelanggan.


**_Transaksi Pembelian vs Pembelian_**
"""

# Mengilustrasikan grup berdasarkan beberapa fitur dalam scatterplot

plt.figure(figsize=(14,10))
X = data_kmeans['PembelianTRX']
Y = data_kmeans['Pembelian']
g = sns.scatterplot(x=X, y=Y, hue = data_kmeans['Labels'], palette = 'rocket')
g.set_xlabel('Transaksi Pembelian')
g.set_ylabel('Pembelian per Akun')
g.set_title('Transaksi Pembelian vs Pembelian')
plt.show()

plt.savefig("Transaksi Pembelian vs Pembelian KMeans.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dari grafik terlihat dari 4 cluster, cluster Pengguna dengan kredit pembelian tinggi (pink) terdapat transaksi pembelian sampai melebihi 350 dan pembelian per akun melebihi 40000. Sedangkan untuk ketiga cluster lainnya pembelian per akun hanya kurang dari 10000

**_Saldo Akun vs Pembelian_**
"""

plt.figure(figsize=(14,10))
X = data_kmeans['Saldo']
Y = data_kmeans['Pembelian']
g = sns.scatterplot(x=X, y=Y, hue = data_kmeans['Labels'], palette = 'rocket')
g.set_xlabel('Saldo Akun')
g.set_ylabel('Pembelian per Akun')
g.set_title('Saldo Akun vs Pembelian')
plt.show()

plt.savefig("Saldo Akun vs Pembelian KMeans.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dari grafik terlihat bahwa Cluster pengguna tingkat tengah (ungu muda) dan cluster pengguna dengan kredit pembelian tinggi (pink) masing-masing memiliki saldo yang besar, tetapi mereka berbeda dalam hal pembelian per akun. Cluster pengguna tingkat tengah (ungu muda) terlihat tidak banyak melakukan pembelian, sedangkan cluster pengguna dengan kredit pembelian tinggi (pink) terlihat banyak melakukan pembelian.

**_Tingkat Frekuensi  Pembelian vs Transaksi Pembelian_**
"""

plt.figure(figsize=(14,10))
X = data_kmeans['FrekuensiPembelian']
Y = data_kmeans['PembelianTRX']
g = sns.scatterplot(x=X, y=Y, hue = data_kmeans['Labels'], palette = 'rocket')
g.set_xlabel('Tingkat Frekuensi  Pembelian')
g.set_ylabel('Transaksi Pembelian')
g.set_title('Tingkat Frekuensi  Pembelian vs Transaksi Pembelian')
plt.show()

plt.savefig("Tingkat Frekuensi  Pembelian vs Transaksi Pembelian KMeans.png")

"""Berdasarkan grafik tersebut, kelompok pengguna dengan penarikan tunai tinggi (warna ungu tua) dan kelompok pengguna level tengah (warna ungu muda) rata-rata memiliki tingkat frekuensi pembelian mulai dari 0,0 hingga 0,7, dengan transaksi yang kurang dari 100.

Namun, kelompok pengguna dengan kredit hemat (merah) dan kelompok pengguna dengan kredit pembelian tinggi (pink) rata-rata memiliki tingkat frekuensi pembelian 0,3â€“1,0, dengan jumlah transaksi yang mungkin lebih dari 100.


**_Batas Kredit Akun vs Pembelian per Akun_**
"""

plt.figure(figsize=(14,10))
X = data_kmeans['BatasKredit']
Y = data_kmeans['Pembelian']
g = sns.scatterplot(x=X, y=Y, hue = data_kmeans['Labels'], palette = 'rocket')
g.set_xlabel('Batas Kredit Akun')
g.set_ylabel('Pembelian per Akun')
g.set_title('Batas Kredit Akun vs Pembelian per Akun')
plt.show()

plt.savefig("Batas Kredit Akun vs Pembelian per Akun KMeans.png",
            bbox_inches ="tight",
            pad_inches = 1,
            orientation ='landscape')

"""Dari grafik terlihat bahwa rata-rata cluster memiliki batas kredit akun kurang lebih 20000 serta terdapat beberapa yang  lebih. Namun cluster  Pengguna dengan kredit pembelian tinggi (pink) dengan batas kredit tersebut tetap melakukan banyak pembelian melebihi 10000 bahkan terdapat beberapa yang melebihi 30000

## b. PCA (Principal Component Analysis)

- Melihat nilai variance ratio dari pca
"""

pca.explained_variance_ratio_

"""- Visualisai Principal Components Variance untuk melihat nilai dari components beserta besarnya variance yang ditangkap"""

# Memvisualisasikan pada diagram plot
plt.figure(figsize=(12,10))
plt.plot(range(1, 18), pca.explained_variance_ratio_.cumsum(), marker='X', linestyle='--', )
plt.title('Variance PCA yang Ditangkap')
plt.xlabel('Principal Components')
plt.ylabel('Variance yang Ditangkap')
plt.show()

"""Berdasarkan diagram tersebut, kita dapat membandingkan berbagai komponen yang ada di dalamnya, seperti dengan Algoritma K-Means, tidak ada kompponen pemotong yang jelas. Kita dapat mencoba untuk menetapkan batas pada 80% varians dan menyimpan 7 komponen agar mampu mempertahankan analisis.

- Menjalankan ulang PCA dengan nilai principal components yang sudah dipilih
"""

pca = PCA(n_components=7, random_state = 42)
pca.fit(scaled_df)
pca.components_

pca_df = pd.DataFrame(data = pca.components_, columns = df.columns.values,
                      index = ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7'])
pca_df

"""- Visualisasi components menggunakan heatmap (opsional)"""

plt.figure(figsize=(16,10))
sns.heatmap(pca_df, vmin=-1, vmax=1, cmap='Set1', annot=True)
plt.title('Principal Component Feature Correlations')
plt.yticks([0, 1, 2, 3, 4, 5, 6], ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7'],rotation=0, fontsize=9)
plt.show()

"""Berdasarkan diagram heatmap tersebut, nilai korelasi tertinggi yatu pada angka 0.44 yaitu nilai milik PCA2 yang berkorelasi dengan penarikan tunai dan PCA4 yang berkorelasi dengan Jangka Waktu."""

pca_scores = pca.transform(scaled_df)
pca_scores

"""- Melihat dimensi dari pca_scores (opsional)"""

pca_scores.shape

"""- Melakukan clustering ulang pada data component PCA yang sudah diubah dengan menggunakan KMeans"""

wcss = []
for i in range(1, 18):
    kmeans_pca = KMeans(n_clusters=i, init='k-means++')
    kmeans_pca.fit(pca_scores)
    wcss.append(kmeans_pca.inertia_)

"""- Visualisasi untuk melihat jumlah cluster yang cocok dengan menggunakan elbow method"""

# Divisualisasikan dalam diagram plot
plt.figure(figsize=(16,10))
plt.plot(range(1, 18), wcss, marker='o', linestyle='dotted')
plt.xlabel('Jumlah Cluster')
plt.ylabel('WCSS Score')
plt.title('K-Means dengan PCA Components')
plt.show()

"""Berdasarkan visualisasi, jumlah cluster yang dimulai dari empat dan seterusnya memiliki nilai atau jarak optimal. Namun, dalam hal ini, kami memilih nilai cluster sebanyak empat karena kami mempertimbangkan biaya perhitungan saat menentukan jumlah cluster, karena biaya perhitungan akan meningkat jika jumlah cluster ditambahkan. Disarankan untuk menggunakan jumlah cluster yang lebih sedikit jika kita tidak memiliki sumber daya komputasi yang banyak.

- Melatih model untuk Kmeans PCA dengan n_cluster sebanyak 4
"""

kmeans_pca = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans_pca.fit(pca_scores)

"""- Menamahakan nilai segment KMeans PCA dan nilai component pada dataframe dengan membuat kolom baru"""

pca_kmeans = pd.concat([df.reset_index(drop=True), pd.DataFrame(pca_scores)], axis=1)
pca_kmeans.columns.values[-7:] = ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7']

pca_kmeans['K-Means PCA Segment'] = kmeans_pca.labels_
pca_kmeans

"""Dataframe sudah memiliki original features, jumlah PCA Component tetap, dan kolom label segmen yang sesuai dengan clustering KMeans yang diterapkan ke jumlah grup yang tetap.

- Melakukan grouping berdasarkan segmennya dan melihat mean dari semua feaures dan PCA Components
"""

pca_kmeans_freq = pca_kmeans.groupby(['K-Means PCA Segment']).mean()
pca_kmeans_freq

"""Setelah menerapkan ulang model KMeans, kami perlu memilih nama yang sesuai untuk PCA Component. Atribut akan mengikuti pola yang sama dengan data tanpa PCA Component, jadi kami memilih nama segmen yang sama seperti sebelumnya di sini.

- Melihat Perolehan jumlah observasi (pengamatan) dan proporsi observasi (pengamatan) dalam setiap segmen
"""

pca_kmeans_freq['JumlahPengamatan'] = pca_kmeans[['K-Means PCA Segment', 'Saldo']].groupby(['K-Means PCA Segment']).count()
pca_kmeans_freq['ProporsiPengamatan'] = pca_kmeans_freq['JumlahPengamatan'] / pca_kmeans_freq['JumlahPengamatan'].sum()
pca_kmeans_freq = pca_kmeans_freq.rename({0:'Pengguna Level Tengah',
                                          1:'Pengguna dengan Penarikan Tunai Tinggi',
                                          2:'Pengguna Kredit Hemat',
                                          3:'Penggnua dengan Pembelian Kredit Tinggi'})
pca_kmeans_freq

"""- Visualisasi Jumlah Observasi (Pengamatan) per segment"""

pca_kmeans_freq.plot.pie(y='JumlahPengamatan', figsize=(6, 6), cmap='CMRmap')
plt.legend(loc='center')
plt.title('Jumlah Observasi (Pengamatan) per Segment')
plt.show()

"""Berdasarkan diagram Pie di atas, setiap segment memiliki jumlah record yang berbeda. Urutangrup juga diubah berdasarkan K-Means Clustering

- Membuat visualisasi plot beberapa PCA Component satu sama lain dengan memberi warna pada segmen KMeans
"""

pca_kmeans['Legend'] = pca_kmeans['K-Means PCA Segment'].map({0:'Pengguna Level Tengah',
                                                              1:'Pengguna dengan Penarikan Tunai Tinggi',
                                                              2:'Pengguna Kredit Hemat',
                                                              3:'Pengguna dengan Pembelian Kredit Tinggi'})

"""Karena terdapat beberapa PCA Component, jadi akan sedikit rumit untuk merencanakan dan menyimpan semua kemungkinan kombinasi komponen di sini. maka kami akan memplot komponen yang dipilih.

- **Plotting PCA Components**
"""

# Plotting perbandingan PCA 1 dan PCA 2

X = pca_kmeans['PCA1']
Y = pca_kmeans['PCA2']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 1 vs PCA Component 2 Clusters')
plt.show()

plt.savefig("PCA Component 1 vs PCA Component 2 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 2 dan PCA 3

X = pca_kmeans['PCA2']
Y = pca_kmeans['PCA3']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 2 vs PCA Component 3 Clusters')
plt.show()

plt.savefig("PCA Component 2 vs PCA Component 3 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 3 dan PCA 4

X = pca_kmeans['PCA3']
Y = pca_kmeans['PCA4']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 3 vs PCA Component 4 Clusters')
plt.show()

plt.savefig("PCA Component 3 vs PCA Component 4 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 4 dan PCA 5

X = pca_kmeans['PCA4']
Y = pca_kmeans['PCA5']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 4 vs PCA Component 5 Clusters')
plt.show()

plt.savefig("PCA Component 4 vs PCA Component 5 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 5 dan PCA 6

X = pca_kmeans['PCA5']
Y = pca_kmeans['PCA6']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 5 vs PCA Component 6 Clusters')
plt.show()

plt.savefig("PCA Component 5 vs PCA Component 6 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 6 dan PCA 7

X = pca_kmeans['PCA6']
Y = pca_kmeans['PCA7']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 6 vs PCA Component 7 Clusters')
plt.show()

plt.savefig("PCA Component 6 vs PCA Component 7 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 2 dan PCA 5

X = pca_kmeans['PCA2']
Y = pca_kmeans['PCA5']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 2 vs PCA Component 5 Clusters')
plt.show()

plt.savefig("PCA Component 2 vs PCA Component 5 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 1 dan PCA 6

X = pca_kmeans['PCA1']
Y = pca_kmeans['PCA6']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 1 vs PCA Component 6 Clusters')
plt.show()

plt.savefig("PCA Component 1 vs PCA Component 6 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

# Plotting perbandingan PCA 2 dan PCA 7

X = pca_kmeans['PCA2']
Y = pca_kmeans['PCA7']
plt.figure(figsize=(19, 8))
sns.scatterplot(x=X, y=Y, hue = pca_kmeans['Legend'], palette='CMRmap')
plt.title('PCA Component 2 vs PCA Component 7 Clusters')
plt.show()

plt.savefig("PCA Component 2 vs PCA Component 7 Clusters.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""# UMAP APPLICATION
UMAP, atau Uniform Manifold Approximation and Projection, adalah algoritma pengurangan dimensi yang efisien dalam analisis data. Tujuan utamanya adalah memetakan data dari ruang dimensi tinggi ke ruang dimensi yang lebih rendah, menjaga struktur dan jarak antar titik yang signifikan. UMAP sering digunakan untuk visualisasi data kompleks, memungkinkan identifikasi pola kluster, dan membantu penelitian dalam pemahaman struktur data yang kompleks

Dalam uji coba ini, kita butuh 7 komponen agar bisa mencakup sekitar 80% variasi data. Namun, kita mau mencoba cara yang lebih baik dengan menggunakan UMAP untuk mengurangi dimensi data, serta membentuk kelompok dengan cara yang berbeda. Setelah itu, kita akan lihat apakah UMAP bisa membuat dataset ini lebih sederhana menjadi hanya 2 dimensi.

Sebelumnya, kita harus pilih berapa banyak tetangga untuk setiap titik data. Kalau kita pilih lebih banyak tetangga, kita bisa melihat data secara lebih menyeluruh.

*   Melihat 5 baris dataset teratas
"""

df.head()

"""- Melakukan standarisasi pada dataset"""

scaler = StandardScaler()
scaledDf = scaler.fit_transform(df)
scaledDf

"""
Setelah itu, kita akan membuat gambaran visual untuk melihat dataset yang sudah diubah menjadi hanya 2 dimensi,

- Menentukan model dari UMAP"""

umap_data = UMAP(n_neighbors=100,
                 n_components=2, # Untuk menganalisa jika bisa dilakukan split data pada 2 components
                 metric='euclidean',
                 n_epochs=1000,
                 learning_rate=0.1, #1.0
                 init='spectral',
                 min_dist=0.1,
                 spread=1.0,
                 low_memory=False,
                 set_op_mix_ratio=1.0,
                 local_connectivity=1,
                 repulsion_strength=1.0,
                 negative_sample_rate=5,
                 transform_queue_size=4.0,
                 random_state=3,
                 angular_rp_forest=False,
                 target_n_neighbors=-1,
                 transform_seed=3,
                 verbose=False,
                 unique=False,
                )

umap_data

"""- Melatih model dari UMAP data"""

umap_fit = umap_data.fit_transform(scaledDf)
umap_fit

"""- Melihat dimensi dari UMAP data"""

umap_fit.shape

"""- Menyertakan hasil dari UMAP ke dalam dataframe dengan cara membuat kolom baru"""

data_umap = pd.concat([df, pd.DataFrame(umap_fit, columns=['UMAP1', 'UMAP2'])], axis=1)

data_umap.head()

"""- Memeriksa apakah ada nilai yang hilang (missing value) dalam data UMAP."""

data_umap.isnull().any()

"""- Menjalankan algoritma K-Means kembali pada data yang telah diubah dari hasil penerapan UMAP, yang sekarang hanya memiliki 2 komponen"""

wcss = []
for i in range(1, 18):
    kmeans_pca = KMeans(n_clusters=i, init='k-means++', random_state=3)
    kmeans_pca.fit(umap_fit)
    wcss.append(kmeans_pca.inertia_)

"""- Membuat gambaran visual untuk menampilkan jumlah kelompok (cluster) dari hasil K-Means pada data UMAP"""

plt.figure(figsize=(16,10))
plt.plot(range(1, 18), wcss, marker='X', linestyle='-.')
plt.xlabel('Jumlah Cluster')
plt.ylabel('WCSS Score')
plt.title('K-Means dengan UMAP Data')
plt.show()

"""Dengan memanfaatkan K-Means pada data UMAP, kita dapatkan jumlah kelompok optimal sebanyak 4, berdasarkan plot Scree. Meski begitu, penggunaan 4 kelompok memberikan nilai skor WCSS yang lebih rendah dibandingkan ketika diterapkan pada komponen PCA.

- Melakukan pelatihan pada model K-Means UMAP dengan parameter n_clusters = 4
"""

kmeans_umap = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans_umap.fit(umap_fit)

"""- Menyisipkan segmen hasil K-Means UMAP ke dalam dataframe"""

data_umap_kmeans = pd.concat([df.reset_index(drop=True), pd.DataFrame(umap_fit)], axis=1)
data_umap_kmeans.columns.values[-2:] = ['UMAP1', 'UMAP2']

data_umap_kmeans['K-Means UMAP Segment'] = kmeans_umap.labels_
data_umap_kmeans

"""- Memeriksa distribusi data di setiap klaster (segmen K-Means UMAP)"""

data_umap_kmeans['K-Means UMAP Segment'].value_counts()

"""- Meninjau frekuensi rata-rata dari segmen K-Means UMAP dengan melakukan pengelompokan (grouping)"""

umap_kmeans_freq = data_umap_kmeans.groupby(['K-Means UMAP Segment']).mean()
umap_kmeans_freq

"""- Memberikan label pada klaster dengan cara yang sama seperti sebelumnya, kemudian mengeksplorasi beberapa variabel berdasarkan klaster yang dihasilkan oleh segmen UMAP"""

data_umap_kmeans['Labels'] = data_umap_kmeans['K-Means UMAP Segment'].map({0:'Pengguna yang Bergantung Pada Uang Muka',
                                                                           1:'Pengguna dengan Pembelian dan Kredit Besar',
                                                                           2:'Pengguna Normal (Standard)',
                                                                           3:'Pengguna dengan Cicilan Pembelian dan Minimal Kredit'})

data_umap_kmeans.head()

"""- Menjelajahi klaster UMAP K-Means melalui eksplorasi data

**Penarikan Tunai vs Pembelian Angsuran**
"""

plt.figure(figsize=(14,10))
X = data_umap_kmeans['PenarikanTunai']
Y = data_umap_kmeans['PembelianAngsuran']
g = sns.scatterplot(x=X, y=Y, hue = data_umap_kmeans['Labels'], palette = 'Spectral')
g.set_xlabel('Penarikan Tunai')
g.set_ylabel('Pembelian Angsuran')
g.set_title('Penarikan Tunai vs Pembelian Angsuran')
plt.show()

plt.savefig("Penarikan Tunai vs Pembelian Angsuran UMAP.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dari grafik, terlihat bahwa orang yang cenderung mengandalkan uang muka, ditunjukkan dengan warna hijau muda, bisa melakukan banyak pembelian dengan pembayaran angsuran tanpa perlu menarik tunai atau menggunakan uang muka lewat kredit.

Sementara itu, orang yang biasanya melakukan pembelian besar dan menggunakan kredit, ditandai dengan warna oranye, cenderung memiliki penggunaan uang muka atau penarikan tunai yang lebih tinggi. Ini berarti mereka sangat mengandalkan uang muka tunai, meskipun berada pada ujung bawah pembelian dengan angsuran.

**Pembelian vs Pembayaran**
"""

plt.figure(figsize=(14,10))
X = data_umap_kmeans['Pembelian']
Y = data_umap_kmeans['Pembayaran']
g = sns.scatterplot(x=X, y=Y, hue = data_umap_kmeans['Labels'], palette = 'Spectral')
g.set_xlabel('Pembelian')
g.set_ylabel('Pembayaran')
g.set_title('Pembelian vs Pembayaran')

plt.show()

plt.savefig("Pembelian vs Pembayaran UMAP.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dari grafik, terlihat bahwa pengguna yang lebih mengandalkan uang muka (warna hijau muda) umumnya melakukan pembelian dan pembayaran dalam jumlah yang melebihi 40000. Di sisi lain, pengguna dengan pembelian angsuran dan kredit minimum (warna merah) serta pengguna dengan pembelian besar dan kredit tinggi (warna oranye) hampir memiliki pola persebaran yang serupa, di mana keduanya cenderung memiliki pembayaran yang tinggi, sekitar 40000.

Sementara itu, pengguna kategori normal (warna biru) memiliki kecenderungan untuk melakukan pembelian dan pembayaran yang lebih kecil, yaitu kurang dari 10000.

**Pembelian Satu Kali (one off) vs Batas Kredit**
"""

plt.figure(figsize=(14,10))
X = data_umap_kmeans['PembelianOneoff']
Y = data_umap_kmeans['BatasKredit']
g = sns.scatterplot(x=X, y=Y, hue = data_umap_kmeans['Labels'], palette = 'Spectral')
g.set_xlabel('Pembelian Satu Kali (one off)')
g.set_ylabel('Batas Kredit')
g.set_title('Pembelian Satu Kali (one off) vs Batas Kredit')

plt.show()

plt.savefig("Pembelian Satu Kali (one off) vs Batas Kredit UMAP.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dari grafik, terlihat bahwa semua kelompok atau segmen rata-rata memiliki batas kredit sekitar 24000 dengan pembelian satu kali sekitar 11000. Tetapi, pada kelompok pengguna yang lebih mengandalkan uang muka (warna hijau muda), terdapat beberapa pengguna dengan batas kredit lebih dari 24000 dan pembelian satu kali lebih dari 11000.

**Saldo vs Minimal / Minimum Pembayaran**
"""

plt.figure(figsize=(14,10))
X = data_umap_kmeans['Saldo']
Y = data_umap_kmeans['MinimalPembayaran']
g = sns.scatterplot(x=X, y=Y, hue = data_umap_kmeans['Labels'], palette = 'Spectral')
g.set_xlabel('Saldo')
g.set_ylabel('Minimal / Minimum Pembayaran')
g.set_title('Saldo vs Minimal / Minimum Pembayaran')

plt.show()

plt.savefig("Saldo vs Minimum Pembayaran UMAP.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dari grafik, terlihat bahwa kelompok pengguna yang cenderung mengandalkan uang muka (warna hijau muda) memiliki saldo yang tinggi, mirip dengan kelompok pengguna yang melakukan pembelian besar dengan kredit (orange). Penting untuk dicatat bahwa kelompok pengguna yang mengandalkan uang muka (hijau muda) memiliki pembayaran minimum yang lebih tinggi. Hal ini sesuai karena kelompok ini melakukan pembelian dengan jumlah paling banyak dibandingkan dengan kelompok lainnya.

- Membuat diagram lingkaran (pie chart) yang serupa dengan data menggunakan PCA untuk menampilkan visualisasi Jumlah Pengamatan per Segmen
"""

umap_kmeans_freq['JumlahPengamatan'] = data_umap_kmeans[['K-Means UMAP Segment', 'Saldo']].groupby(['K-Means UMAP Segment']).count()
umap_kmeans_freq['ProporsiPengamatan'] = umap_kmeans_freq['JumlahPengamatan'] / umap_kmeans_freq['JumlahPengamatan'].sum()
umap_kmeans_freq = umap_kmeans_freq.rename({0:'Pengguna yang Bergantung Pada Uang Muka',
                                            1:'Pengguna dengan Pembelian dan Kredit Besar',
                                            2:'Pengguna Normal (Standard)',
                                            3:'Pengguna dengan Cicilan Pembelian dan Minimal Kredit'})
umap_kmeans_freq

umap_kmeans_freq.plot.pie(y='JumlahPengamatan', figsize=(6, 6), autopct='%1.0f%%', cmap = 'Spectral')
plt.ylabel(None, loc='center')
plt.legend(loc='center right')
plt.title('Jumlah Pengamatan per Segment')
plt.show()

plt.savefig("Jumlah Pengamatan per Segment UMAP.png",
            bbox_inches ="tight",
            pad_inches = 1,
            transparent = True,
            orientation ='landscape')

"""Dengan memanfaatkan UMAP sebagai teknik pengurangan dimensi dengan skema K-Means yang identik, ditemukan bahwa kelompoknya lebih seimbang dibandingkan dengan penggunaan PCA.

## a. Model Inference

Pada tahap ini, kita akan menggunakan model Random Forest untuk melakukan inferensi, di mana model yang telah dilatih akan diuji pada data selain data yang telah ada. Data yang dimaksud dapat berupa data buatan oleh siswa atau data yang ditemukan di internet.

Model inferensi yang digunakan adalah Random Forest, di mana prediksi akan dilakukan berdasarkan input yang diberikan untuk menentukan klaster mana yang sesuai.

Variabel yang akan digunakan dalam proses ini adalah `Saldo`, `PenarikanTunai`, `PembelianAngsuran`, `Pembelian`, `Pembayaran`, `PembelianOneoff`, `BatasKredit`, `MinimalPembayaran`, dan `JangkaWaktu`.

- Melihat informasi dataset
"""

data = data_umap_kmeans.copy()
data.info()

"""- Menghilangkan kolom-kolom yang tidak dibutuhkan"""

drop_cols = ['FrekuensiSaldo', 'FrekuensiPembelian', 'FrekuensiPembelianOneoff', 'FrekuensiPembelianAngsuran',
             'FrekuensiPenarikanTunai', 'PenarikanTunaiTRX', 'PembelianTRX', 'PembayaranFullPRC', 'UMAP1', 'UMAP2',
             'K-Means UMAP Segment']

data = data.drop(drop_cols, axis=1)
data.head()

"""- Memulai inisialisasi variabel X dan y. Di sini, kita mendefinisikan X sebagai kolom-kolom kecuali `Labels`, sedangkan y sebagai kolom `Labels`"""

X = data.drop(['Labels'], axis=1)
y = data['Labels']

print(X.shape)
print(y.shape)

"""- Memisahkan data menjadi data Pelatihan (Training) dan Pengujian (Testing) menggunakan kolom atau atribut yang telah diperoleh. Proporsi data Pelatihan adalah 80% (0.80), sedangkan data Pengujian adalah 20% (0.20)

"""

X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y, test_size=0.20, random_state=13)

print(X_train.shape)
print(X_test.shape)

"""- Menentukan model dan melatih model klasifikasi Random Forest dengan metode GAUSSIAN CLASSIFIER dan menggunakan 100 estimator"""

clf = RandomForestClassifier(n_estimators=100)

RFC = clf.fit(X_train, y_train)
RFC

"""- Mengevaluasi kinerja dengan menggunakan `.predict_proba()`, yang menghasilkan matriks probabilitas bahwa keluaran yang diprediksi sama dengan nol atau sat"""

RFC.predict_proba(X_test)

"""- Melihat predicted outputs dengan .predict()"""

y_pred = RFC.predict(X_test)
y_pred

"""- Mengamati nilai akurasi dengan menggunakan `.score`"""

print("Test score:", RFC.score(X_test, y_test))
print("Train score:", RFC.score(X_train, y_train))

"""Diperoleh hasil nilai skor untuk data pengujian sebesar 84% (0.838), sementara untuk data pelatihan nilai skornya mencapai 100% (1.0)

- Melihat matriks kebingungan (confusion matrix) dan laporan klasifikasi (classification report) dari model
"""

confusion_matrix(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", linewidths=.5, cmap = 'winter')
plt.show()
print(classification_report(y_test, y_pred))

"""- Mengetahui summary dari evaluasi model"""

print('Ringkasan Evaluasi Model Random Forest Classifier')
print('Akurasi :', accuracy_score(y_test, y_pred))
print('Presisi :', precision_score(y_test,y_pred, average='macro'))
print('Recall :', recall_score(y_test,y_pred, average='macro'))
print('F1 Score :', f1_score(y_test,y_pred, average='macro'))

"""Berdasarkan evaluasi model, diperoleh akurasi dari model Random Forest Classifier sebesar 84% (0.838), yang menunjukkan hasil yang baik dan dapat dianggap akurat. Oleh karena itu, model ini dapat digunakan untuk melakukan penentuan kluster pengguna.

- Melaksanakan prediksi untuk menentukan kluster
"""

print('--- PREDIKSI PENENTUAN CLUSTER PENGGUNA KARTU KREDIT MENGGUNAKAN RANDOM FOREST ---')
print('='*100)

saldo = float(input('Jumlah Saldo Pengguna (exp : 40.9321) = '))
pembelian = float(input('Jumlah Pembelian yang Dilakukan Pengguna (exp : 16.0) = '))
pembelianoneoff = float(input('Banyaknya Satu Kali Pembelian yang Dilakukan Pengguna (exp : 16.0) = '))
pembelianangsuran = float(input('Jumlah Pembelian Angsuran yang Dilakukan Pengguna (exp : 95.0) = '))
penarikantunai = float(input('Jumlah Penarikan Tunai yang Dilakukan Pengguna (exp : 205.23) = '))
bataskredit = float(input('Jumlah Batas Kredit Pengguna (exp : 7000.0) = '))
pembayaran = float(input('Jumlah Pembayaran yang Harus Dibayarkan Pengguna (exp : 312.87) = '))
minimalpembayaran = float(input('Jumlah Minimal Pembayaran yang Harus Dibayarkan Pengguna (exp : 1072.87) = '))
jangkawaktu = int(input('Jangka Waktu Pengguna Kredit (inputan : 6/7/8/9/10/11/12) = '))

val = [saldo, pembelian, pembelianoneoff, pembelianangsuran, penarikantunai, bataskredit, pembayaran,
       minimalpembayaran, jangkawaktu]
print(val)

predict_rfc = RFC.predict([val])
print('='*100)
print('Berdasarkan analisa, pengguna masuk kedalam cluster : ', predict_rfc)

"""Dari hasil analisis, dapat diambil beberapa informasi mengenai pengguna berdasarkan data input yang diberikan:

1. **Jumlah Saldo Pengguna:** 50.14328
2. **Jumlah Pembelian yang Dilakukan Pengguna:** 24
3. **Banyaknya Satu Kali Pembelian yang Dilakukan Pengguna:** 2
4. **Jumlah Pembelian Angsuran yang Dilakukan Pengguna:** 86
5. **Jumlah Penarikan Tunai yang Dilakukan Pengguna:** 109.09
6. **Jumlah Batas Kredit Pengguna:** 7893
7. **Jumlah Pembayaran yang Harus Dibayarkan Pengguna:** 345.9
8. **Jumlah Minimal Pembayaran yang Harus Dibayarkan Pengguna:** 50
9. **Jangka Waktu Pengguna Kredit:** 11

Hasil analisis menunjukkan bahwa pengguna termasuk dalam kluster 'Pengguna dengan Pembelian dan Kredit Besar' berdasarkan pola dan karakteristik data yang dimasukkan.

## b. Save and Finalize Model

Di sini, kami memanfaatkan operasi Pickle untuk menyimpan format serialisasi ke dalam suatu file dan kemudian menggunakannya untuk membuat prediksi baru.
"""

pickle.dump(RFC, open('model_FP4.pkl', 'wb'))

"""# KESIMPULAN

Dari evaluasi yang telah kami lakukan, kami mendapatkan kesimpulan sebagai berikut:

- Dalam analisis ini, kami menggunakan model KMeans, PCA, dan UMAP Application untuk melakukan pengelompokan data pengguna kartu kredit.
- Pada KMeans, kami memilih 4 klaster berdasarkan observasi dari elbow method.
- Pada model PCA, kami memutuskan untuk menggunakan 7 komponen untuk menganalisis data dengan tujuan mempertahankan setidaknya 80% dari varians yang terkandung dalam data. Selanjutnya, dengan bantuan UMAP Application, kami mereduksi jumlah komponen menjadi 2 dimensi.
- Alasan kami memilih UMAP Application adalah karena kemampuannya dalam menemukan kesamaan dan mempertahankan struktur global data. Dengan menggunakan 2 komponen berdasarkan UMAP, kami berhasil mendapatkan klaster yang lebih seimbang dan masih dapat menangkap perbedaan signifikan di antara mereka.
- Hasil analisis menggunakan elbow method menunjukkan bahwa jumlah klaster 4 merupakan jumlah optimal. Klaster atau label yang terbentuk melibatkan Pengguna yang Bergantung pada Uang Muka sebanyak 31%, Pengguna dengan Pembelian dan Kredit Besar sebanyak 30%, Pengguna Normal (Standard) sebanyak 17%, dan Pengguna dengan Cicilan Pembelian dan Minimal Kredit sebanyak 22%.
- Untuk prediksi klaster pengguna, kami menggunakan model klasifikasi random forest dan memperoleh tingkat akurasi sebesar 84%.
- Variabel independen yang kami gunakan untuk prediksi meliputi `Saldo`, `PenarikanTunai`, `PembelianAngsuran`, `Pembelian`, `Pembayaran`, `PembelianOneoff`, `BatasKredit`, `MinimalPembayaran`, dan `JangkaWaktu`. Sementara itu, variabel dependen atau target yang digunakan adalah `Labels`.
- Dalam analisis ini, kami menyertakan visualisasi menggunakan lineplot, boxplot, histogram, pie chart, dan matriks korelasi (heatmap). Selain itu, kami juga melakukan contoh penggunaan groupby dan query terhadap data.
- Untuk mengatasi nilai yang hilang (missing value), kami memilih untuk mengisi nilai yang kosong dengan median.
"""