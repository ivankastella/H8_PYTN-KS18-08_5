# -*- coding: utf-8 -*-
"""PYTN_KampusMerdeka_fp2_MuhamadAdityaDarmawan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/adityadarmawann/PYTN_KampusMerdeka_fp2_MuhamadAdityaDarmawan/blob/master/PYTN_KampusMerdeka_fp2_MuhamadAdityaDarmawan.ipynb

# FINAL PROJECT 2 :   
**" Logistic Regression dan SVM "**

Nama Anggota Kelompok :
- MUHAMAD ADITYA DARMAWAN (PYTN-KS18-01)
- AUDITA BELLA INTAN PUSPITA (PYTN-KS18-05)
- IVANKA STELLA AUDRIA (PYTN-KS18-08)

# Pendahuluan

## Latar Belakang

Cuaca yaitu salah satu faktor alam yang paling berpengaruh dalam kehidupan. berbagai kegiatan dan aktivitas manusia akan bergantung pada cuaca pada hari itu. Untuk itu diperlukan perkiraan cuaca atau prediksi cuaca untuk mengetahui cuaca pada hari itu agar manusia dapa menjalankan aktivitasnya. Adapun faktor-faktor yang dapat mempengaruhi cuaca diantaranya adalah suhu, kelembaban, tekanan udara, kecepatan angin, dan lain sebagainya. Salah satu fenomena penting bagi kehidupan yang dipengaruhi oleh cuaca adalah hujan. Berdasarkan permasalahan itu, perlu dilakukansebuah analisis dengan suatu model yang bisa digunakan untuk  memprediksi terjadinya hujan agar masyarakat dapat menjalankan aktivitas dengan lebih baik.

## Dataset
Dataset yang digunakan pada analisis ini yaitu Rain in Australia yang diunduh dari kaggle melalui [link berikut](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)

Dataset ini berisi observasi harian perkiraan hujan di seluruh daerah Australia selama 10 tahun. Dataset ini terdiri dari 23 atribut dan 145460 baris. Atribut yang menjadi variabel target adalah RainTomorrow yang berisi "No" (tidak terjadi hujan besok) dan "Yes" (Terjadi hujan besok 1mm atau lebih).

## Objective

Objective yang ingin dicapai dalam analisis ini, yaitu:

- Mengetahui faktor-faktor yang berpengaruh signifikan dalam memprediksi hujan di Australia.
- Menemukan model terbaik untuk digunakan dalam memprediksi hujan di Australia.
- Memberikan insight yang dapat membantu pihak penduduk dalam bentuk charts.

#Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
# Library for Preprocessing

import pandas as pd
import numpy as np

# Library for Visualization

import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
# %matplotlib inline

# Library for Encoding Categorical Data

from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Library for Splitting Data

from sklearn.model_selection import train_test_split, cross_val_score

# Library for Building Model

from sklearn.linear_model import  LogisticRegression
from sklearn import svm
from sklearn.svm import SVC
from sklearn.svm import LinearSVC

# Library for Evaluating Model

from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

# Library for Ignore Warnings

import warnings
warnings.filterwarnings('ignore')

"""#Data Loading

### Import Dataset
"""

# Load dataset weatherAUS.csv sebagai dataframe

df = pd.read_csv("weatherAUS.csv")

# Menampilkan value data teratas dari dataframe

df.head()

# Menampilkan data terbawah

df.tail()

# Melihat Jumlah Baris dan Kolom

df.shape

"""### Type of Data"""

# Menampilkan informasi dari value column dataframe

df.info()

"""### View Data Statistical"""

# Menampilkan Deskripsi dari Dataframe yang telah dilakukan loading

df.describe().T

# Melihat Informasi lebih detail mengenai struktur DataFrame kolom kategorikal (objek)

df.describe(include=object).T

"""### Calculate Unique Values"""

# Menghitung nilai unique pada data

df.nunique()

"""### Check Missing Values"""

# Checking Missing Value

df.isnull().sum()

"""Banyak mengandung missing value"""

#   Melihat Persentasi Missing Value
for col in df.columns:
    count_null = df[col].isnull().sum()
    total = len(df[col])
    presentase = (count_null/total)*100

    print(f'Kolom {col}')
    print(f'Jumlah null {count_null}')
    print(f'Presentase null : {presentase:.2f}%\n')

"""### Check Data Duplikasi"""

# Cek duplikasi data
df.duplicated().sum()

"""Tidak mengandung data terduplikasi

# Data Cleaning

### Handling Missing Values
"""

# Mengcopy dataframe untuk dilakukan cleaning

df_clean = df.copy()

# Menampilkan baris teratas

df_clean.head()

"""#### Hapus missing value yang presentasinya diatas 20%
Penghapusan kolom yang memiliki missing value diatas 20 persen dilakukan karena akan mengganggu pada mode. Adapun kolom yang dihapus sebagai berikut:
1. Sunshine dengan perentasi missing value 48.01%
2. Evaporation dengan presentasi missing value 43.17%
3. Cloud9am dengan presentase missing value 38.17%
4. Cloud3pm dengan presentase missing value 40.81%
"""

# Menghapus missing value yang lebih dari 20%

df_clean.drop(['Sunshine', 'Evaporation', 'Cloud9am', 'Cloud3pm'], axis=1, inplace=True)
df_clean.head()

# Cek informasi column pada dataframe

df_clean.info()

# Cek kembali kolom yang mempunyai missing value

df_clean.isnull().sum()

# Mengisi missing value dengan nilai mean

float_cols = df_clean.select_dtypes(include = ['float']).columns
cat_var = ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']
for col in float_cols:
    df_clean[col] = df_clean[col].fillna(df_clean[col].mean())
for col in cat_var:
    df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])

df_clean.isnull().sum()

# Drop missing value pada target

df_clean = df_clean[df_clean['RainTomorrow'].notna()]
df_clean.isnull().sum()

df_clean.info()

"""Terjadi perubahan yang size awalnya 25,5 MB menjadi 21,7 MB

# Data Exploration

Explorasi dilakukan dengan mencari variable x yang memiliki korelasi terhadap variable target 'RainTomorrow'
"""

# Cek kembali matrix yang berkorelasi

# Memilih hanya kolom numerik

df_clean_num = df_clean.select_dtypes(include=['int64', 'float64'])

# Membuat hitmap

fig,ax = plt.subplots(figsize=(20,20))
cm = sns.heatmap(df_clean_num.corr(), linewidths= .5, annot=True, fmt='.2f')

df_clean.head()

"""### Analisa Rain Tomorrow"""

# Analisa Rain Tomorrow sebagai Target

rain_tomorrow_type = df_clean['RainTomorrow'].value_counts(normalize=True)
rain_tomorrow_type

# Pemodelan bentuk bar

rain_tomorrow_type.plot(kind = 'bar',
                    figsize=(20,10),
                    color='green',
                    legend = False)

# nama judul bar chart
plt.title('Distribusi kolom rain tomorrow', fontsize = 20)
# nama xlabel
plt.xlabel('Nilai kolom', fontsize = 15, color = "red")
# nama ylabel
plt.ylabel('Jumlah', fontsize = 15, color = "red")

# print bar chart
plt.show()

"""- Pada kolom rain tomorrow terdapat 2 value yaitu yes dan no
- Yes menunjukkan besok akan hujan
- No menunjukkan besok tidak akan hujan
- Distrubusi value yes sebesar 22.4%
- Distribusi value no sebesar 77.6%

### Analisasi Pengaruh Kolom Date terhadap Rain Tomorrow
"""

# Pendefinisian Korelasi

corel_date = df_clean.loc[: , ['Date', 'RainTomorrow']]
corel_date

# Encode

label_encode = preprocessing.LabelEncoder()

corel_date_encode = corel_date.copy()

for col in corel_date.select_dtypes(include='O').columns:
    corel_date_encode[col] = label_encode.fit_transform(corel_date[col])

# Membuat heatmap korelasi data dengan rain tomorrow

corel_matriks_MM = corel_date_encode.corr()
plt.figure(figsize=(10,8))
sns.heatmap(corel_matriks_MM, cmap='Greens', annot=True, annot_kws={'fontsize':12})
plt.show()

"""Pada saat pengecekan korelasi nilai, kolom date korelasinya sangat kecil sehingga tidak perlu dijadikan features.

### Pengaruh Rainfall terhadap Rain Tomorrow
"""

# Encoder Rain Tomorrow

label_encoder = preprocessing.LabelEncoder()
df_clean['RainTomorrow']= label_encoder.fit_transform(df_clean['RainTomorrow'])

# Cek Korelasi Rainfall dengan Rin Tomorrow

korelasi_rainfall = df_clean.loc[:, [ 'Rainfall', 'RainTomorrow']]
korelasi_rainfall

matriksKorelasi = korelasi_rainfall.corr()
plt.figure(figsize=(10,8))
sns.heatmap(matriksKorelasi, cmap='Greens', annot=True, annot_kws={'fontsize':12})
plt.show()

"""Kolom Rainfall memiliki korelasi positif terhadap rain tomorrow, besar kemungkinan mempengaruhi hasil dari rain tomorrow

## Pengaruh Kolom yang mengandung kata Wind Terhadap Rain Tomorrow
Pada beberapa kolom mengandung unsur wind mulai dari kecepatan angin, arah angin, dan waktu terjadinya angin tersebut. Hal tersebut akan dikelompokkan menjadi sebuah variabel yang mengandung unsur unsur tersebut untuk di analisa.

### Mengubah data arah angin dengan busur derajat arah angin
Data WindGustDir, WindDir3pm dan WindDir9am merupakan data sebuah arah angin. Data data tersebut bisa diubah menjadi sebuah derajat pada mata angin.
"""

df_clean['WindDir9am'].unique()

df_clean['WindDir3pm'].unique()

df_clean['WindGustDir'].unique()

arah_angin = {'N': 0, 'NNE': 22.5, 'NE': 45, 'ENE': 67.5,
              'E': 90, 'ESE': 112.5, 'SE': 135, 'SSE': 157.5,
              'S': 180, 'SSW': 202.5, 'SW': 225, 'WSW': 247.5,
              'W': 270, 'WNW': 292.5, 'NW': 315, 'NNW': 337.5}
#pada ubah data
df_clean['WindGustDir'] = df_clean['WindGustDir'].map(arah_angin)
df_clean['WindDir9am'] = df_clean['WindDir9am'].map(arah_angin)
df_clean['WindDir3pm'] = df_clean['WindDir3pm'].map(arah_angin)

"""## Slicing Data Unsur Wind"""

# Melihat Korelasi Wind

korelasi_wind = df_clean.loc[:, [ 'WindGustDir', 'WindGustSpeed', 'WindDir9am','WindDir3pm','WindSpeed9am', 'WindSpeed3pm','RainTomorrow']]
korelasi_wind

"""### Tabel Korelasi wind dengan rain tomorrow"""

# Memodelkan korelasi menggunakan heatmap

matriksKorelasi = korelasi_wind.corr()
plt.figure(figsize=(10,8))
sns.heatmap(matriksKorelasi, cmap='Greens', annot=True, annot_kws={'fontsize':12})
plt.show()

"""### Kesimpulan Kolom yang mengandung kata Wind dengan Rain Tomorrow
- Kolom yang mengandung unsur wind berjumlah 6 kolom
- Kolom WindGustDir, WindDir9am, dan WindDir3pm adalah kolom mengenai Arah angin
- Kolom WindGustSpeed, WindSpeed9am, dan WindSpeed3pm adalah kolom mengenai kecepatan angin
- Korelasi paling tinggi terhadap rain tomorrow adalah kolom WindGustSpeed. jadi wind gust speed sudah cukup mewakili kolom dengan unsur angin yang lain
- Wind GustDir memiliki korelasi yang rendsah sehingga tidak digunakan sebagai feature

- Kami menyimpulkan kolom wind gust speed sudah cukup untuk mewakili kolom kolom lain

## Pengaruh 'MinTemp', 'MaxTemp', 'Temp9am', dan 'Temp3pm' Terhadap RainTomorrow
Dilakukan analisa apakah kolom 'MinTemp', 'MaxTemp', 'Temp9am', dan 'Temp3pm' mempengaruhi kolom 'RainTomorrow'
Kami menggabungkan beberapa kolom yang mengandung unsur temp karena berkaitan dengan suhu

### Rata rata 'MinTemp', 'MaxTemp', Temp9am, dan Temo3pm terhadap Rain Tomorrow
"""

# Melakukan Grouping Rata Rata

df_clean.groupby("RainTomorrow")[["MinTemp","MaxTemp", "Temp9am", "Temp3pm"]].mean()

"""### Scatter Plot 'Temp9am' dan 'Temp3pm' terhadap Rain Tomorrow"""

# Membuat Diagram Scatter Plot Temp 3pm dan 9am dengan RainTomorrow

plt.figure(figsize=(8,6))
sns.scatterplot(x="Temp9am", y="Temp3pm", hue = "RainTomorrow", data=df_clean)
plt.title("Hubungan Temp 3pm dan 9am dengan RainTomorrow")

"""### Scatter Plot 'MinTemp' dan 'MaxTemp' terhadap Rain Tomorrow"""

# Membuat Scatter plot untuk MinMax Temperature dengan Rain Tomorrow

plt.figure(figsize=(8,6))
sns.scatterplot(x="MinTemp", y="MaxTemp", hue = "RainTomorrow", data=df_clean)
plt.title("Hubungan MinMaxTemperature dengan RainTomorrow")

"""### Heat map MinTemp, MaxTemp, Temp9am, Temp3pm dan RainTomorrow

"""

# Memodelkan korelasi

korelasi_MinMaxTemp = df_clean.loc[:, [ 'MinTemp', 'MaxTemp', "Temp9am", "Temp3pm", 'RainTomorrow']]
korelasi_MinMaxTemp

# Encode
label_encode = preprocessing.LabelEncoder()

# Menyalin data pada variabel cuaca_encode
korelasiMinMaxTemp_encode = korelasi_MinMaxTemp.copy()

for col in korelasi_MinMaxTemp.select_dtypes(include='O').columns:
    korelasiMinMaxTemp_encode[col]=label_encode.fit_transform(korelasi_MinMaxTemp[col])

korelasiMinMaxTemp_encode

# Menampilkan Heatmap untuk matriks korelasi

matriksKorelasi = korelasiMinMaxTemp_encode.corr()
plt.figure(figsize=(10,8))
sns.heatmap(matriksKorelasi, cmap='Greens', annot=True, annot_kws={'fontsize':12})
plt.show()

"""### Kesimpulan korelasi MinTemp MaxTemp dan RainTomorrow
- MinTemp adalah kolom berisikan temperature terendah hari itu
- MaxTemp adalah temperature tertinggi pada hari itu
- Pada heatmap didapatkan bahwa korelasi MinTemp dan RainTomorrow berkorelasi positif
- Pada heatmap korelasi MaxTemp dan RainTomorrow berkorelasi negatif
- Pada heatmap didapatkan korelasi yang lemah terhadap rain romorrow

Kami simpulkan bahwa kolom kolom temp9am, dan temp3pm tidak mempengaruhi hujan pada esok hari jadi tidak dijadikan sebagai feature. Sedangkan pada kolom MinTemp dan Max Temp sudah mewakili kolom temp9am dan 3pm sehingga yang digunakan sebagai feature adalah MinTemp dan MaxTemp

## Pengaruh Location Terhadap RainTomorrow
Dilakukan analisa apakah kolom Location' apakah mempengaruhi kolom 'RainTomorrow'
"""

# Menampilkan Unique Value pada kolom location
df_clean['Location'].unique()

# Menghitung total dari setiap unique pada kolom 'hour'
sum_location = df_clean.groupby('Location').size()
sum_location

# Hitung jumlah kejadian "RainTomorrow" yang sama dengan "Yes" untuk setiap lokasi

df_rain_tomorrow_yes = df.loc[df['RainTomorrow']=='Yes'].groupby('Location')['RainTomorrow'].count().reset_index(name='Yes')
df_rain_tomorrow_yes = df_rain_tomorrow_yes.sort_values(by='Yes', ascending=False)

# Buat figure dengan ukuran 10 x 6 inci
fig, ax = plt.subplots(figsize=(20, 6))

# Buat line chart untuk jumlah Yes
ax.bar(df_rain_tomorrow_yes['Location'], df_rain_tomorrow_yes['Yes'], label='Yes',color='red')



# Atur judul, label sumbu-x dan sumbu-y
ax.set_title('Line Chart Menunjukkan Jumlah RainTomorrow Yes dan No pada Tiap Lokasi')
ax.set_xlabel('Nama Kota')
ax.set_ylabel('Jumlah')

# Atur label untuk setiap line chart
ax.legend()

# Putar label sumbu-x supaya mudah dibaca
plt.xticks(rotation=90)

print(" Top 5 YES ")
print(df_rain_tomorrow_yes.head())
plt.show()

"""**Kesimpulan Location dan RainTomorrow**  

Pada kolom location berisikan data kota kota pada australia. Data kota tersebut berisikan nama kota yang berhubungan dengan kolom RainTomorrow. Dalam nama kota tersebut didapatkan bahwa terjadinya hujan pada setiap kota berbeda beda. Jadi kami simpulkan Location menjadi feature pada model yang akan dibuat karena location mempengaruhi pada raintomorrow

## Pengaruh 'Humidity9am' dan 'Humidity3pm' Terhadap RainTomorrow
Dilakukan analisa apakah kolom 'Humidity9am' dan 'Humidity3pm' mempengaruhi kolom 'RainTomorrow'. Kolom tersebut kami pilih karena sama sama mengandung unsur humidity, Akan tetapi jamnya saja yang berbeda
"""

# Cek Korelasi Humidity

korelasi_humidity = df_clean.loc[:, [ 'Humidity9am', 'Humidity3pm', 'RainTomorrow']]
korelasi_humidity

# Melihat korelasi dengan heatmap

matriksKorelasi = korelasi_humidity.corr()
plt.figure(figsize=(10,8))
sns.heatmap(matriksKorelasi, cmap='Greens', annot=True, annot_kws={'fontsize':12})
plt.show()

"""**Kesimpulan Humidity3pm, humidity9am dan RainTomorrow**  
Pada kolom Humidity pukul 9 dan pukul 3 menunjukkan korelasi positif, akan tetapi pada kolom kolom lain yang mengandung unsur jam 9 dan 3 kebanyakan korelasinya tidak akurat sehingga tidak diambil sebagai feature.

## Pengaruh 'Pressure9am' dan 'Pressure3pm' Terhadap RainTomorrow
Dilakukan analisa apakah kolom 'Pressure9am' dan 'Pressure3pm' mempengaruhi kolom 'RainTomorrow'
"""

# Melihat korelasi kolom yang mengandung Pressure

korelasi_pressure = df_clean.loc[:, [ 'Pressure9am', 'Pressure3pm', 'RainTomorrow']]
korelasi_pressure

# Visualisasi matriks korelsi pada diagram Heatmap

matriksKorelasi = korelasi_pressure.corr()
plt.figure(figsize=(10,8))
sns.heatmap(matriksKorelasi, cmap='Greens', annot=True, annot_kws={'fontsize':12})
plt.show()

"""**Kesimpulan Pressure dan RainTomorrow**  
Pada kolom pressure3pm dan pressure 9am didapatkan sebuah kesimpulan bahwa mereka memiliki korelasi yang rendah terhadap prediksi rain tomorrow, jadi kami menyimpulkan tidak menjadikan pressure 9am dan pressure3pm sebagai feature krn koerlasinya negatif

## Pengaruh RainToday terhadap RainTomorrow
"""

# Pendefinisian rain today

rain_today_type = df_clean['RainToday'].value_counts(normalize=True)
rain_today_type

# inisiasi bentuk bar
rain_today_type.plot(kind = 'bar',
                    figsize=(20,10),
                    color='green',
                    legend = False)

# nama judul bar chart
plt.title('Banyaknya nilai yes dan no pada kolom "rain today"', fontsize = 20)
# nama xlabel
plt.xlabel('Nilai kolom', fontsize = 15, color = "red")
# nama ylabel
plt.ylabel('Jumlah', fontsize = 15, color = "red")

# print bar chart
plt.show()

"""**Keterangan**
- Pada kolom rain today berisikan data yes dan no. yes mewakili hujan dan no tidak
- Presentase hujan sebesar 77.8%
- Presentase jumlah tidak hujan sebesar 22.2%
- Jika dibandingkan dengan presentase kolom rain tomorrow sebagai berikut :
    - Kolom rain tomorrow bernilai no berjumlah 77.6%
    - Kolom rain tomorrow bernilai yes berjumlah 22.4%
- Jumlah presentase rain tomorrow dan rain today sangatlah mirip jadi menurut kami data tersebut berkorelasi

# **Kesimpulan EDA**
Kolom kolom yang digunakan sebagai feature sebagai berikut :
- Rainfall
- Location
- WindGustSpeed
- MinTemp
- MaxTemp
- Rain Today


Kolom kolom tersebut dipilih sebagaimana alasan pada keterangan tiap analisa sebelumnya

# Central of Tendency
"""

# Mencari Mean

mean_gustspeed = df_clean['WindGustSpeed'].mean()

print('Mean dari kolom price', mean_gustspeed)

# Mencari Median

median_gustspeed = df_clean['WindGustSpeed'].median()

print('Median dari kolom price', median_gustspeed)

# Mencari Modus
mode_gustspeed = df_clean['WindGustSpeed'].mode()

print('Mode dari kolom name', mode_gustspeed)

"""# Distribusi Kolom Location"""

plt.figure(figsize=(10,6))
sns.histplot(data=df_clean, x="Location", kde=True)
plt.title("Distribusi kolom 'Locatino' ")

# Putar label sumbu-x supaya mudah dibaca
plt.xticks(rotation=90)

plt.show()

"""# Data Preprocessing

Data preprocessing merupakan suatu proses untuk melakukan proses sebelum membuat sebuah model. Data data tersebut di proses berdasarkan explorasi data yang sebelumnya dilakukan.
"""

df_clean.info()

"""Berdasarkan eksplorasi data sebelumnya, kita telah mengidentifikasi beberapa variabel kunci yang signifikan untuk analisis, antara lain:

1. Location:
Kolom "Location" mencakup informasi lokasi kejadian.

2. Temperature (Temp):
Kolom "Temp" terdiri dari "MinTemp," "MaxTemp," "Temp9am," dan "Temp3pm." Diantara ini, "MinTemp" dan "MaxTemp" memiliki korelasi tinggi dan cukup mewakili "Temp9am" dan "Temp3pm." Oleh karena itu, kita memilih "MinTemp" dan "MaxTemp" sebagai fitur utama terkait suhu.

3. Wind:
Variabel "Wind" terdiri dari beberapa kolom yang mencakup elemen-elemen angin seperti "WindGustDir," "WindGustSpeed," "WinDir9am," "WindDir9pm," "WindSpeed9am," dan "WindSpeed3pm." Korelasi tinggi terdapat pada "WindGustSpeed," sehingga kita memilihnya sebagai fitur representatif untuk elemen angin.

4. Humidity:
Terdapat dua jenis kelembapan, yaitu "Humidity9am" dan "Humidity3pm." Meskipun keduanya memiliki korelasi yang lumayan tinggi, variabel lain seperti "WindSpeed3pm" dengan unsur jam memiliki korelasi rendah. Oleh karena itu, kita memilih hanya menggunakan "Humidity9am" dan "Humidity3pm" sebagai fitur.

5. Pressure:
Kolom "Pressure" terdiri dari dua jenis, yaitu "Pressure9am" dan "Pressure3pm." Namun, korelasi keduanya rendah sehingga tidak dijadikan sebagai fitur.

6. Rain Today:
Kolom "Rain Today" menunjukkan hasil yang mirip dengan "Rain Tomorrow" secara presentase, sehingga kita memilihnya sebagai fitur yang relevan.

Dengan demikian, fitur yang dipilih untuk analisis meliputi: "Location," "Rainfall," "WindGustSpeed," "MinTemp," "MaxTemp," dan "Rain Today."
"""

data_modeling = df_clean.loc[:, ['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'RainToday', 'RainTomorrow']]
data_modeling

"""## Encode"""

# Menentukan Kategori Variable
categoric = [var for var in data_modeling.columns if data_modeling[var].dtype=='O']

print('There are {} categorical variables\n'.format(len(categoric)))

print('The categorical variables are :', categoric)

# Menerapkan One Hot Encoding pada Variabel kategorik
data_modeling = pd.get_dummies(data_modeling, columns=['Location', 'RainToday'],
                            prefix=["loc", 'rain'],
                            drop_first=True)
data_modeling

"""## Outlier"""

# draw boxplots to visualize outliers

plt.figure(figsize=(15,10))

plt.subplot(2, 2, 1)
fig = data_modeling.boxplot(column='MinTemp')
fig.set_title('')
fig.set_ylabel('MinTemp')


plt.subplot(2, 2, 2)
fig = data_modeling.boxplot(column='MaxTemp')
fig.set_title('')
fig.set_ylabel('MaxTemp')

plt.subplot(2, 2, 3)
fig = data_modeling.boxplot(column='Rainfall')
fig.set_title('')
fig.set_ylabel('WindSpeed9am')


plt.subplot(2, 2, 4)
fig = data_modeling.boxplot(column='WindGustSpeed')
fig.set_title('')
fig.set_ylabel('WindGustSpeed')

"""## Outlier MinTemp"""

sorted(data_modeling['MinTemp'])

# Membuat quantile

quantile1, quantile3= np.percentile(data_modeling['MinTemp'],[25,75])

## Menemukan IQR
iqr_value=quantile3-quantile1

print('nilai quartile 1:', quantile1)
print('nilai quartile 3:', quantile3)
print('nilai iqr', iqr_value)

# Menentukan batas atas dan batas bawah

lower_bound_val = quantile1 -(1.5 * iqr_value)
upper_bound_val = quantile3 +(1.5 * iqr_value)

print(lower_bound_val,upper_bound_val)

# Menghitung data yang outlier pada limit bawah

(data_modeling['MinTemp'] < lower_bound_val).sum()

# Menghitung data yang outlier pada limit atas

(data_modeling['MinTemp'] > upper_bound_val).sum()

# Dilakukan filter pada data untuk membuang data yang outlier

model_no_outlier = data_modeling[(data_modeling['MinTemp']>lower_bound_val) & (data_modeling['MinTemp'] < upper_bound_val)]

"""## Outlier MaxTemp"""

sorted(data_modeling['MaxTemp'])

# Memodelkan pada quantile

quantile1, quantile3= np.percentile(data_modeling['MaxTemp'],[25,75])

# Menemukan IQR
iqr_value=quantile3-quantile1

print('nilai quartile 1:', quantile1)
print('nilai quartile 3:', quantile3)
print('nilai iqr', iqr_value)

# Menentukan batas bawah dan batas atas

lower_bound_val = quantile1 -(1.5 * iqr_value)
upper_bound_val = quantile3 +(1.5 * iqr_value)

print(lower_bound_val,upper_bound_val)

# Menghitung data yang outlier pada limit bawah

(data_modeling['MaxTemp'] < lower_bound_val).sum()

# Menghitung data yang outlier pada limit atas

(data_modeling['MaxTemp'] > upper_bound_val).sum()

# Dilakukan filter pada data untuk membuang data yang outlier

model_no_outlier = data_modeling[(data_modeling['MaxTemp']>lower_bound_val) & (data_modeling['MaxTemp'] < upper_bound_val)]

"""## Outlier Rainfall"""

sorted(data_modeling['Rainfall'])

# Membuat quantile

quantile1, quantile3= np.percentile(data_modeling['Rainfall'],[25,75])

# Menentukan quantile
iqr_value=quantile3-quantile1

print('nilai quartile 1:', quantile1)
print('nilai quartile 3:', quantile3)
print('nilai iqr', iqr_value)

# Menemukan batas atas dan batas bawah

lower_bound_val = quantile1 -(1.5 * iqr_value)
upper_bound_val = quantile3 +(1.5 * iqr_value)

print(lower_bound_val,upper_bound_val)

# Menghitung data yang outlier pada limit bawah

(data_modeling['Rainfall'] < lower_bound_val).sum()

# Menghitung data yang outlier pada limit atas

(data_modeling['Rainfall'] > upper_bound_val).sum()

# Dilakukan filter pada data untuk membuang data yang outlier

model_no_outlier = data_modeling[(data_modeling['Rainfall']>lower_bound_val) & (data_modeling['Rainfall'] < upper_bound_val)]

"""## Outlier WindGustSpeed"""

sorted(data_modeling['WindGustSpeed'])

# Menentukan Quantile

quantile1, quantile3= np.percentile(data_modeling['WindGustSpeed'],[25,75])

## Menemukan IQR
iqr_value=quantile3-quantile1

print('nilai quartile 1:', quantile1)
print('nilai quartile 3:', quantile3)
print('nilai iqr', iqr_value)

# Menemukan batas atas dan batas akhir

lower_bound_val = quantile1 -(1.5 * iqr_value)
upper_bound_val = quantile3 +(1.5 * iqr_value)

print(lower_bound_val,upper_bound_val)

# Menghitung data yang outlier pada limit bawah

(data_modeling['WindGustSpeed'] < lower_bound_val).sum()

# Menghitung data yang outlier pada limit atas

(data_modeling['WindGustSpeed'] > upper_bound_val).sum()

# Dilakukan filter pada data untuk membuang data yang outlier

model_no_outlier = data_modeling[(data_modeling['WindGustSpeed']>lower_bound_val) & (data_modeling['WindGustSpeed'] < upper_bound_val)]

"""## Setelah Outlier Handling"""

# draw boxplots to visualize outliers

plt.figure(figsize=(15,10))

plt.subplot(2, 2, 1)
fig = model_no_outlier.boxplot(column='MinTemp')
fig.set_title('')
fig.set_ylabel('MinTemp')


plt.subplot(2, 2, 2)
fig = model_no_outlier.boxplot(column='MaxTemp')
fig.set_title('')
fig.set_ylabel('MaxTemp')


plt.subplot(2, 2, 3)
fig = model_no_outlier.boxplot(column='Rainfall')
fig.set_title('')
fig.set_ylabel('WindSpeed9am')


plt.subplot(2, 2, 4)
fig = model_no_outlier.boxplot(column='WindGustSpeed')
fig.set_title('')
fig.set_ylabel('WindGustSpeed')

"""# Model Defiation

Metode pengujian yang akan digunakan pada project kali ini adalah metode Logistic Regression dengan pembanding SVM

## Logistic Regression
Metode pengujian pada training ini menggunakan metode regresi logistik. Alasan penggunaan metode karena regresi logistik dapat dipakai untuk masalah klasifikasi biner. Model akan menghitung probabilitas data masing-masing kelas kemudian membuat keputusan akhir dengan mengambil probabilitas tertinggi.

## SVM
Model kedua yakni model SVM. Model ini digunakan sebagai pembanding model regresi logistik. SVM dapat mengolah data berdimensi tinggi tanpa mengalami penurunan performa yang signifikan.
"""

logistic = LogisticRegression()
svm = LinearSVC()

"""# Evaluasi Model

### SVM
"""

x_svm = model_no_outlier.drop(['RainTomorrow',"Rainfall"], axis = 1)
y_svm = model_no_outlier['RainTomorrow']

# Training dan test
x_train_svm, x_test_svm, y_train_svm, y_test_svm = train_test_split(x_svm ,y_svm, random_state=42, test_size=0.3)

# Training
svm.fit(x_train_svm, y_train_svm)

"""## Score Evaluasi SVM"""

# Membuat prediksi svm

predict_svm = svm.predict(x_test_svm)

# Mengukur Akurasi SVM

akurasi_svm = accuracy_score(y_test_svm, predict_svm)
print('Accuracy score Test - Model SVM: {0:0.4f}'. format(akurasi_svm))

# Mencetak hasil

print(confusion_matrix(y_test_svm, predict_svm))
print(classification_report(y_test_svm, predict_svm))
print(accuracy_score(y_test_svm, predict_svm))

"""### Cross Validation"""

# Menerapkan scoring validasi
score_cross_svm = cross_val_score(svm, x_train_svm, y_train_svm, cv = 5, scoring='accuracy')

# Menghitung score cross-validation
print('Cross-validation scores:{}'.format(score_cross_svm))

# Menghitung rata - rata score cross-validation
print('Average cross-validation score: {:.4f}'.format(score_cross_svm.mean()))

"""### Confusion Matrix"""

# Membuat diagram subplots

svm_confusion = confusion_matrix(y_test_svm, predict_svm)

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(svm_confusion)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, svm_confusion[i, j], ha='center', va='center', color='red')
plt.show()

"""Confusion matrix diatas menunjukkan prediksi benar and  prediksi salah

In this case, we have
- `True Positives` (Actual Positive:1 and Predict Positive:1)


- `True Negatives` (Actual Negative:0 and Predict Negative:0)


- `False Positives` (Actual Negative:0 but Predict Positive:1)  `(Type I error)`


- `False Negatives` (Actual Positive:1 but Predict Negative:0)  `(Type II error)`
"""

# Mencetak Hasil

print('Ringkasan Evaluasi Model SVM')
print('Akurasi : {0:0.4f}'. format(akurasi_svm))
print('Presisi : {0:0.4f}'. format(precision_score(y_test_svm,predict_svm)))
print('Recall : {0:0.4f}'. format( recall_score(y_test_svm,predict_svm)))
print('F1 Score : {0:0.4f}'. format(f1_score(y_test_svm,predict_svm)))

"""## Logistic Regression"""

x_logistic = model_no_outlier.drop(['RainTomorrow',"Rainfall"], axis = 1)
y_logistic = model_no_outlier['RainTomorrow']

# Training dan test

x_train_log, x_test_log, y_train_log, y_test_log = train_test_split(x_logistic ,y_logistic, random_state=42, test_size=0.3)

# Training

logistic.fit(x_train_log,y_train_log)

"""### Hasil Score Logistic Regression"""

# Mendefinisikan prediksi logistik

predict_logistic = logistic.predict(x_test_log)

# Membuat akurasi prediksi logistik

accuracy_score_logistic = accuracy_score(y_test_log,predict_logistic)
print('accuracy :', accuracy_score_logistic)

print(classification_report(y_test_log,predict_logistic))

# Membuat diagram sublplot logistik regression

logistic_regression_confusion = confusion_matrix(y_test_log, predict_logistic)

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(logistic_regression_confusion)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, logistic_regression_confusion[i, j], ha='center', va='center', color='red')
plt.show()

"""Confusion matrix diatas menunjukkan `prediksi benar` and ` prediksi salah`.

In this case, we have
- `True Positives` (Actual Positive:1 and Predict Positive:1)


- `True Negatives` (Actual Negative:0 and Predict Negative:0)


- `False Positives` (Actual Negative:0 but Predict Positive:1)  `(Type I error)`


- `False Negatives` (Actual Positive:1 but Predict Negative:0)  `(Type II error)`
"""

print('Ringkasan Evaluasi Model Logistic Regression')
print('Akurasi : {0:0.4f}'. format(accuracy_score_logistic))
print('Presisi : {0:0.4f}'. format(precision_score(y_test_log, predict_logistic)))
print('Recall : {0:0.4f}'. format( recall_score(y_test_log, predict_logistic)))
print('F1 Score : {0:0.4f}'. format(f1_score(y_test_log, predict_logistic)))

"""## Perbandingan SVM dengan Logistic Regression"""

print('Ringkasan Evaluasi Model SVM')
print('Akurasi : {0:0.4f}'. format(akurasi_svm))
print('Presisi : {0:0.4f}'. format(precision_score(y_test_svm,predict_svm)))
print('Recall : {0:0.4f}'. format( recall_score(y_test_svm,predict_svm)))
print('F1 Score : {0:0.4f}'. format(f1_score(y_test_svm,predict_svm)))

print('Ringkasan Evaluasi Model Logistic Regression')
print('Akurasi : {0:0.4f}'. format(accuracy_score_logistic))
print('Presisi : {0:0.4f}'. format(precision_score(y_test_log, predict_logistic)))
print('Recall : {0:0.4f}'. format( recall_score(y_test_log, predict_logistic)))
print('F1 Score : {0:0.4f}'. format(f1_score(y_test_log, predict_logistic)))

"""## Hasil
Berdasarkan evaluasi model, terlihat bahwa baik model Regresi Logistik (LR) maupun model Support Vector Machine (SVM) memiliki tingkat akurasi yang hampir identik, masing-masing mencapai 81% dan 79% untuk logistic regression dan SVM. Perbedaan antara keduanya hanya sekitar 1%. Meskipun demikian, ketika menilik nilai presisi dan recall, keduanya menunjukkan keunggulan masing-masing dalam memprediksi kelas yang berbeda. SVM menonjol dengan nilai presisi yang lebih tinggi untuk kelas positif (fraud), sementara Regresi Logistik memiliki recall yang lebih tinggi untuk kelas positif.

Dalam konteks ini, performa model LR dan SVM menunjukkan keseimbangan yang baik antara data pelatihan dan pengujian, dengan perbedaan skor yang tidak signifikan. Oleh karena itu, dapat disimpulkan bahwa model tidak mengalami overfitting atau underfitting. Meskipun skor akurasi dan precision keduanya cukup tinggi (> 0.6), namun skor recall dan F1 masih tergolong rendah.
"""

# Menampilkan kembali hasil SVM

x_svm.head()

# Logistic Regression

test_logistic = {'MinTemp':[20, 18.3, 11, 13.3],
         'MaxTemp':[28, 30, 32, 21],
         'WindGustSpeed':[33, 50, 90, 20],

         'loc_Albany': [0, 0, 0, 0],
         'loc_Albury': [0, 0, 0, 0],
         'loc_AliceSprings': [0, 0, 0, 0],
         'loc_BadgerysCreek': [0, 0, 0, 0],
         'loc_Ballarat': [0, 0, 0, 0],
         'loc_Bendigo': [0, 0, 0, 0],
         'loc_Brisbane': [0, 0, 0, 0],
         'loc_Brisbane': [0, 0, 0, 0],
         'loc_Cairns': [0, 0, 0, 0],
         'loc_Canberra': [0, 0, 0, 0],
         'loc_Cobar': [0, 0, 0, 0],
         'loc_CoffsHarbour': [0, 0, 0, 0],
         'loc_Dartmoor': [0, 0, 0, 0],
         'loc_Darwin': [0, 0, 0, 0],
         'loc_GoldCoast': [0, 0, 0, 0],
         'loc_Hobart': [0, 0, 0, 0],
         'loc_Katherine': [0, 0, 0, 0],
         'loc_Launceston': [0, 0, 0, 0],
         'loc_Melbourne': [0, 0, 0, 0],
         'loc_MelbourneAirport': [0, 0, 0, 0],
         'loc_Mildura': [0, 0, 0, 0],
         'loc_Moree': [0, 0, 0, 0],
         'loc_MountGambier': [0, 0, 0, 0],
         'loc_MountGinini': [0, 0, 0, 0],
         'loc_Newcastle': [0, 0, 0, 0],
         'loc_Nhil': [0, 0, 0, 0],
         'loc_NorahHead': [0, 0, 0, 0],
         'loc_NorfolkIsland' : [0, 0, 0, 0],
         'loc_Nuriootpa'  : [0, 0, 0, 0],
         'loc_PearceRAAF'  : [0, 0, 0, 0],
         'loc_Penrith'  : [0, 0, 0, 0],
         'loc_Perth'  : [0, 0, 0, 0],
         'loc_PerthAirport'  : [0, 0, 0, 0],
         'loc_Portland'  : [0, 0, 0, 0],
         'loc_Richmond'  : [0, 0, 0, 0],
         'loc_Sale'  : [0, 0, 0, 0],
         'loc_SalmonGums'  : [0, 0, 0, 0],
         'loc_Sydney'  : [0, 0, 0, 0],
         'loc_SydneyAirport' : [0, 0, 0, 0],
         'loc_Townsville'  : [0, 0, 0, 0],
         'loc_Tuggeranong'  : [0, 0, 0, 0],
         'loc_Uluru'  : [0, 0, 0, 0],
         'loc_WaggaWagga'  : [0, 0, 0, 0],
         'loc_Walpole'  : [0, 0, 0, 0],
         'loc_Watsonia'  : [0, 0, 0, 0],
         'loc_Williamtown'  : [1, 0, 0, 0],
         'loc_Witchcliffe'  : [0, 1, 0, 0],
         'loc_Wollongong'  : [0, 0, 1, 0],
         'loc_Woomera'  : [0, 0, 0, 1],
         'rain_Yes' : [0, 1, 0, 1]
        }



test_logistic = pd.DataFrame(test_logistic)
test_logistic

# Menampilkan predict logistic

predict_logistic_test = logistic.predict(test_logistic)
predict_logistic_test

"""# Kesimpulan

## Kesimpulan Model Inference Logistic Regression
Pada model inferensi, terdapat data acak yang sesuai dengan data di data_modeling. Selanjutnya, data tersebut diuji menggunakan prediksi regresi logistik, menghasilkan nilai 0 yang menunjukkan ketiadaan hujan pada hari berikutnya dalam pengujian tersebut.

## Kesimpulan SVM dan Regresi Logistik pada Data WeatherAUS
Dari rangkaian tindakan yang dilakukan pada data weatherAUS, beberapa temuan diperoleh sebagai berikut:

- Data weatherAUS mencakup berbagai jenis informasi yang memengaruhi kondisi hujan di wilayah Australia. Data ini kemudian diproses untuk menentukan fitur yang akan digunakan.
- Hasil analisis EDA menunjukkan bahwa data yang tidak memiliki korelasi tinggi diabaikan, sedangkan data yang memiliki korelasi tinggi dipilih sebagai fitur.
- Skor SVM mencapai 79%, sedangkan skor Regresi Logistik mencapai 81%.
- Perbedaan nilai skor antara SVM dan Regresi Logistik hanya 2%.
- Pengujian dengan model inferensi menghasilkan output [0, 0, 0, 0], menunjukkan ketiadaan hujan pada kedua set data tersebut.  


Berdasarkan pengujian, dapat disimpulkan bahwa hasil Regresi Logistik lebih baik 2% daripada SVM pada data weatherAUS.
"""